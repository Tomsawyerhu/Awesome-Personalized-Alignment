{"Title": "[Args: Alignment as reward-guided search](http://arxiv.org/abs/1912.00180v1)", "Abbr": "Args", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/1912.00180v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Reward-augmented decoding: Efficient controlled text generation with a unidirectional reward model](http://arxiv.org/abs/2310.09520v4)", "Abbr": "RAD", "Year": 2023, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2310.09520v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Cascade reward sampling for efficient decoding-time alignment](http://arxiv.org/abs/2406.16306v3)", "Abbr": "CARDS", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2406.16306v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Deal: Decoding-time alignment for large language models](http://arxiv.org/abs/2410.01079v1)", "Abbr": "DeAL", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2410.01079v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Genarm: Reward guided generation with autoregressive reward model for test-time alignment](http://arxiv.org/abs/2410.08193v5)", "Abbr": "GenARM", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2410.08193v5.pdf)", "Status": "✅ 成功"}
{"Title": "[Pad: Personalized alignment at decoding-time](http://arxiv.org/abs/2211.04198v1)", "Abbr": "PAD", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2211.04198v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Controlled decoding from language models](http://arxiv.org/abs/2212.10938v1)", "Abbr": "CD", "Year": 2023, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2212.10938v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Don’t throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding](http://arxiv.org/abs/1008.1442v1)", "Abbr": "PPO-MCTS", "Year": 2023, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/1008.1442v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Inference-time language model alignment via integrated value guidance](http://arxiv.org/abs/2409.17819v1)", "Abbr": "IVG", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2409.17819v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Value augmented sampling for language model alignment and personalization](http://arxiv.org/abs/2405.06639v1)", "Abbr": "VAS", "Year": 2024, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2405.06639v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Language model personalization via reward factorization](http://arxiv.org/abs/2503.06358v1)", "Abbr": NaN, "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2503.06358v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Persona-judge: Personalized alignment of large language models via token-level self-judgment](http://arxiv.org/abs/2403.17141v3)", "Abbr": "Persona-judge", "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2403.17141v3.pdf)", "Status": "✅ 成功"}
{"Title": "[MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](http://arxiv.org/abs/2208.11125v1)", "Abbr": "MAVIS", "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2208.11125v1.pdf)", "Status": "✅ 成功"}
{"Title": "[PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](http://arxiv.org/abs/2507.07725v1)", "Abbr": "PITA", "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2507.07725v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Parm: Multi-objective test-time alignment via preference-aware autoregressive reward model](http://arxiv.org/abs/2410.08193v5)", "Abbr": "Parm", "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2410.08193v5.pdf)", "Status": "✅ 成功"}
{"Title": "[Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](http://arxiv.org/abs/2504.20458v1)", "Abbr": "GRSU", "Year": 2025, "Source": "arxiv", "Category": "guided decoding", "Tag": "reward-guided decoding", "PDF": "[PDF](http://arxiv.org/pdf/2504.20458v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Evaluating and inducing personality in pre-trained language models](http://arxiv.org/abs/2508.06149v1)", "Abbr": "P2", "Year": 2023, "Source": "NIPS", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2508.06149v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Evaluating character understanding of large language models via character profiling from fictional works.](http://arxiv.org/abs/2404.12726v3)", "Abbr": "Character Profiling", "Year": 2024, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2404.12726v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Whose opinions do language models reflect?](http://arxiv.org/abs/2303.17548v1)", "Abbr": "OpinionQA", "Year": 2023, "Source": "PMLR", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2303.17548v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Do llms understand user preferences? evaluating llms on user rating prediction](http://arxiv.org/abs/2305.06474v1)", "Abbr": NaN, "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2305.06474v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Is chatgpt a good recommender? a preliminary study](http://arxiv.org/abs/2307.03952v3)", "Abbr": NaN, "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2307.03952v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Cue-CoT: Chain-of-thought prompting for responding to in-depth dialogue questions with LLMs](http://arxiv.org/abs/2305.11792v2)", "Abbr": "Cue-CoT", "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2305.11792v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning](http://arxiv.org/abs/2402.10207v6)", "Abbr": "TICL", "Year": 2025, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2402.10207v6.pdf)", "Status": "✅ 成功"}
{"Title": "[ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](http://arxiv.org/abs/2507.09037v1)", "Abbr": "ALIGN", "Year": 2025, "Source": "arxiv", "Category": "prompt-based", "Tag": "Direct Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2507.09037v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Guided profile generation improves personalization with llms](http://arxiv.org/abs/2409.13093v1)", "Abbr": "GPG", "Year": 2024, "Source": "arxiv", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2409.13093v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Integrating summarization and retrieval for enhanced personalization via large language models](http://arxiv.org/abs/2310.20081v1)", "Abbr": NaN, "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2310.20081v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Once:Boostingcontent-basedrecommendationwithbothopen-andclosed-source large language models](http://arxiv.org/abs/2306.07377v1)", "Abbr": "ONCE", "Year": 2024, "Source": "WSDM", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2306.07377v1.pdf)", "Status": "✅ 成功"}
{"Title": "[LLMTreeRec: Unleashing the Power of Large Language Models for Cold-Start Recommendations](http://arxiv.org/abs/2205.13795v1)", "Abbr": "LLMTreeRec", "Year": 2025, "Source": "COLING", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2205.13795v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Towards open-world recommendation with knowledge augmentation from large language models](http://arxiv.org/abs/2306.10933v4)", "Abbr": "KAR", "Year": 2024, "Source": "RecSys", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2306.10933v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Matryoshka: Learning to Drive Black-Box LLMs with LLMs](http://arxiv.org/abs/2310.01957v2)", "Abbr": "Matryoshka", "Year": 2024, "Source": "arxiv", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2310.01957v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Few-shot personalization of llms with mis-aligned re- sponses](http://arxiv.org/abs/1003.3309v1)", "Abbr": "FERMI", "Year": 2024, "Source": "arxiv", "Category": "prompt-based", "Tag": "Profile-Augmented Prompting", "PDF": "[PDF](http://arxiv.org/pdf/1003.3309v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Learning to rewrite prompts for personalized text generation](http://arxiv.org/abs/2310.00152v2)", "Abbr": NaN, "Year": 2024, "Source": "WWW", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2310.00152v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Recgpt: Generative personalized prompts for sequential recommendation via chatgpt training paradigm](http://arxiv.org/abs/2404.08675v1)", "Abbr": "RecGPT", "Year": 2024, "Source": "arxiv", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2404.08675v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized prompt learning for explainable recommendation](http://arxiv.org/abs/2205.09666v3)", "Abbr": "PEPLER-D", "Year": 2023, "Source": "TOIS", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2205.09666v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Graph-enhanced prompt learning for personalized review generation](http://arxiv.org/abs/2505.22447v1)", "Abbr": "GRAPA", "Year": 2024, "Source": "DSE", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2505.22447v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Unlocking the potential of prompt-tuning in bridging generalized and personalized federated learning](http://arxiv.org/abs/2310.18285v4)", "Abbr": "SGPT", "Year": 2024, "Source": "CVPR", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2310.18285v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized federated continual learning via multi-granularity prompt](http://arxiv.org/abs/2407.00113v1)", "Abbr": "PFCL", "Year": 2024, "Source": "KDD", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2407.00113v1.pdf)", "Status": "✅ 成功"}
{"Title": "[The unlocking spell on base llms: Rethink- ing alignment via in-context learning](http://arxiv.org/abs/2312.01552v1)", "Abbr": "URIAL", "Year": 2023, "Source": "ICLR", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2312.01552v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Black-box prompt optimization: Aligning large language models without model training](http://arxiv.org/abs/2307.12980v1)", "Abbr": "BPO", "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2307.12980v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Reinforced prompt personalization for recommendation with large language models](http://arxiv.org/abs/2407.17115v2)", "Abbr": "RPP", "Year": 2025, "Source": "TOIS", "Category": "prompt-based", "Tag": "Personalized-Prompt Prompting", "PDF": "[PDF](http://arxiv.org/pdf/2407.17115v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Rain: Your language models can align themselves without finetuning](http://arxiv.org/abs/2309.07124v2)", "Abbr": "RAIN", "Year": 2023, "Source": "arxiv", "Category": "prompt-based", "Tag": "Prompt-based refine", "PDF": "[PDF](http://arxiv.org/pdf/2309.07124v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Learning to rewrite prompts for personalized text generation](http://arxiv.org/abs/2310.00152v2)", "Abbr": "Rewrite Prompt", "Year": 2024, "Source": "WWW", "Category": "prompt-based", "Tag": "Prompt-based refine", "PDF": "[PDF](http://arxiv.org/pdf/2310.00152v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Lamp: When large language models meet personalization](http://arxiv.org/abs/2304.11406v4)", "Abbr": "Lamp", "Year": 2023.0, "Source": "arxiv", "Category": "RAG", "Tag": "RAG", "PDF": "[PDF](http://arxiv.org/pdf/2304.11406v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs](http://arxiv.org/abs/2409.19401v1)", "Abbr": "EMG-RAG", "Year": 2024.0, "Source": "EMNLP", "Category": "RAG", "Tag": "Indexing", "PDF": "[PDF](http://arxiv.org/pdf/2409.19401v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized Graph-Based Retrieval for Large Language Models](http://arxiv.org/abs/2404.05970v1)", "Abbr": "PGraphRAG", "Year": 2025.0, "Source": "arxiv", "Category": "RAG", "Tag": "Indexing", "PDF": "[PDF](http://arxiv.org/pdf/2404.05970v1.pdf)", "Status": "✅ 成功"}
{"Title": "[MeMemo: on-device retrieval augmentation for private and personalized text generation](http://arxiv.org/abs/2505.00263v1)", "Abbr": "MeMemo", "Year": 2024.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2505.00263v1.pdf)", "Status": "✅ 成功"}
{"Title": "[RECAP: retrieval-enhanced context-aware prefix encoder for personalized dialogue response generation](http://arxiv.org/abs/2408.02271v1)", "Abbr": "RECAP", "Year": 2023.0, "Source": "arxiv", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2408.02271v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Learning retrieval augmentation for personalized dialogue generation](http://arxiv.org/abs/2406.18847v1)", "Abbr": "LAPDOG", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2406.18847v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Partner matters! an empirical study on fusing personas for personalized response selection in retrieval-based chatbots](http://arxiv.org/abs/2310.06390v1)", "Abbr": NaN, "Year": 2021.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2310.06390v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalm: Language model personalization via domain-distributed span aggregated k-nearest n-gram retrieval augmentation](http://arxiv.org/abs/2304.11406v4)", "Abbr": "PersonaLM", "Year": 2023.0, "Source": "EMNLP", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2304.11406v4.pdf)", "Status": "✅ 成功"}
{"Title": "[A personalized dense retrieval framework for unified information access](http://arxiv.org/abs/2304.13654v1)", "Abbr": "UIA", "Year": 2023.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2304.13654v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized retrieval over millions of items](http://arxiv.org/abs/1812.04407v1)", "Abbr": "XPERT", "Year": 2023.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/1812.04407v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Towards personalized and semantic retrieval: An end-to-end solution for e-commerce search via embedding learning](http://arxiv.org/abs/2006.02282v3)", "Abbr": "DPSR", "Year": 2020.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2006.02282v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Learning a fine-grained review-based transformer model for personalized product search](http://arxiv.org/abs/2005.08936v1)", "Abbr": "RTM", "Year": 2021.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2005.08936v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Pearl: Personalizing large language model writing assistants with generation-calibrated retrievers](http://arxiv.org/abs/2306.16641v1)", "Abbr": "Pearl", "Year": 2023.0, "Source": "arxiv", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2306.16641v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Explainable recommendation with personalized review retrieval and aspect learning](http://arxiv.org/abs/2508.20312v2)", "Abbr": "ERRA", "Year": 2023.0, "Source": "arxiv", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2508.20312v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Efficient llm contextualization with user embeddings](http://arxiv.org/abs/2402.13598v2)", "Abbr": "USER-LLM", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Dense", "PDF": "[PDF](http://arxiv.org/pdf/2402.13598v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Integrating summarization and retrieval for enhanced personalization via large language models](http://arxiv.org/abs/2310.20081v1)", "Abbr": "PAG", "Year": 2023.0, "Source": "arxiv", "Category": "RAG", "Tag": "Sparse", "PDF": "[PDF](http://arxiv.org/pdf/2310.20081v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized Graph-Based Retrieval for Large Language Models](http://arxiv.org/abs/2404.05970v1)", "Abbr": "Au et al.", "Year": 2025.0, "Source": "arxiv", "Category": "RAG", "Tag": "Sparse", "PDF": "[PDF](http://arxiv.org/pdf/2404.05970v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems](http://arxiv.org/abs/2204.08128v1)", "Abbr": "UniMS-RAG", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Sparse", "PDF": "[PDF](http://arxiv.org/pdf/2204.08128v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Toward personalized answer generation in e-commerce via multi-perspective preference modeling](http://arxiv.org/abs/2112.13556v1)", "Abbr": "Deng et al.", "Year": 2022.0, "Source": "TOIS", "Category": "RAG", "Tag": "Sparse", "PDF": "[PDF](http://arxiv.org/pdf/2112.13556v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Doing personal laps: Llm-augmented dialogue construction for personalized multi-session conversational search](http://arxiv.org/abs/2405.03480v1)", "Abbr": "LAPS", "Year": 2024.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Prompt-based", "PDF": "[PDF](http://arxiv.org/pdf/2405.03480v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Towards unified multi-modal personalization: Large vision-language models for generative recommendation and beyond](http://arxiv.org/abs/2309.13885v2)", "Abbr": "UniMP", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Prompt-based", "PDF": "[PDF](http://arxiv.org/pdf/2309.13885v2.pdf)", "Status": "✅ 成功"}
{"Title": "[HEART-felt Narratives: Tracing Empathy and Narrative Style in Personal Stories with LLMs](http://arxiv.org/abs/2405.17633v2)", "Abbr": "Shen et al.", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Prompt-based", "PDF": "[PDF](http://arxiv.org/pdf/2405.17633v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Optimization methods for personalizing large language models through retrieval augmentation](http://arxiv.org/abs/2404.05970v1)", "Abbr": "Salemi et al.", "Year": 2024.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Others", "PDF": "[PDF](http://arxiv.org/pdf/2404.05970v1.pdf)", "Status": "✅ 成功"}
{"Title": "[PersonalTM: Transformer memory for personalized retrieval](http://arxiv.org/abs/2402.16288v1)", "Abbr": "PersonalTM", "Year": 2023.0, "Source": "SIGIR", "Category": "RAG", "Tag": "Others", "PDF": "[PDF](http://arxiv.org/pdf/2402.16288v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized LoRA for human-centered text understanding](http://arxiv.org/abs/2403.06208v1)", "Abbr": "Zhang et al.", "Year": 2024.0, "Source": "AAAI", "Category": "RAG", "Tag": "Others", "PDF": "[PDF](http://arxiv.org/pdf/2403.06208v1.pdf)", "Status": "✅ 成功"}
{"Title": "[PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents](http://arxiv.org/abs/2508.03680v1)", "Abbr": "PersonaRAG", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Post-retrieval", "PDF": "[PDF](http://arxiv.org/pdf/2508.03680v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Unims-rag: A unified multi-source retrieval-augmented generation for personalized dialogue systems](http://arxiv.org/abs/2204.08128v1)", "Abbr": "UniMS-RAG", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Post-retrieval", "PDF": "[PDF](http://arxiv.org/pdf/2204.08128v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Learning to Rank for Multiple Retrieval-Augmented Models through Iterative Utility Maximization](http://arxiv.org/abs/2410.09942v2)", "Abbr": "Salemi and Zamani", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Post-retrieval", "PDF": "[PDF](http://arxiv.org/pdf/2410.09942v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Rehearse With User: Personalized Opinion Summarization via Role-Playing based on Large Language Models](http://arxiv.org/abs/2503.00449v1)", "Abbr": "Zhang et al.", "Year": 2025.0, "Source": "arxiv", "Category": "RAG", "Tag": "Post-retrieval", "PDF": "[PDF](http://arxiv.org/pdf/2503.00449v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Fit-rag: black-box rag with factual information and token reduction](http://arxiv.org/abs/2403.14374v1)", "Abbr": "FIT-RAG", "Year": 2024.0, "Source": "arxiv", "Category": "RAG", "Tag": "Post-retrieval", "PDF": "[PDF](http://arxiv.org/pdf/2403.14374v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Conversational health agents: A personalized llm-powered agent framework](http://arxiv.org/abs/2310.02374v5)", "Abbr": NaN, "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2310.02374v5.pdf)", "Status": "✅ 成功"}
{"Title": "[Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models](http://arxiv.org/abs/2410.09541v1)", "Abbr": "RoleLLM", "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2410.09541v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Character-llm: A trainable agent for role-playing](http://arxiv.org/abs/2509.12484v1)", "Abbr": "Character-LLM", "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2509.12484v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews](http://arxiv.org/abs/2310.17976v4)", "Abbr": NaN, "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2310.17976v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Mmrole: A comprehensive framework for developing and evaluating multimodal role-playing agents](http://arxiv.org/abs/2408.04203v2)", "Abbr": "Mmrole", "Year": 2024, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2408.04203v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Capturing minds, not just words: Enhancing role-playing language models with personality-indicative data](http://arxiv.org/abs/2406.18921v3)", "Abbr": NaN, "Year": 2024, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2406.18921v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Enabling conversational interaction with mobile ui using large language models](http://arxiv.org/abs/2209.08655v2)", "Abbr": NaN, "Year": 2023, "Source": "CHI", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2209.08655v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Neeko: Leveraging dynamic lora for efficient multi-character role-playing agent](http://arxiv.org/abs/2509.17676v1)", "Abbr": "Neeko", "Year": 2024, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2509.17676v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Voyager: An open-ended embodied agent with large language models](http://arxiv.org/abs/2406.16294v1)", "Abbr": "VOYAGER", "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2406.16294v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Large language models empowered personalized web agents](http://arxiv.org/abs/2410.17236v2)", "Abbr": "PUMA", "Year": 2025, "Source": "WWW", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2410.17236v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Language models as zero-shot planners: Extracting actionable knowledge for embodied agents](http://arxiv.org/abs/2406.16294v1)", "Abbr": NaN, "Year": 2022, "Source": "PMLR", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2406.16294v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Generative agents: Interactive simulacra of human behavior](http://arxiv.org/abs/2208.04024v1)", "Abbr": NaN, "Year": 2023, "Source": "UIST", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2208.04024v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Agents meet okr: An object and key results driven agent system with hierarchical self-collaboration and self-evaluation](http://arxiv.org/abs/2311.16542v1)", "Abbr": "OKR-Agent", "Year": 2023, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2311.16542v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Investigating the Personality Consistency in Quantized Role-Playing Dialogue Agents](http://arxiv.org/abs/2502.03821v1)", "Abbr": NaN, "Year": 2024, "Source": "EMNLP", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2502.03821v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Socialbench: Sociality evaluation of role-playing conversational agents](http://arxiv.org/abs/2503.17460v2)", "Abbr": "Socialbench", "Year": 2024, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2503.17460v2.pdf)", "Status": "✅ 成功"}
{"Title": "[PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](http://arxiv.org/abs/2506.06254v1)", "Abbr": "PersonaAgent", "Year": 2025, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2506.06254v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personax: A recommendation agent oriented user modeling framework for long behavior sequence](http://arxiv.org/abs/2503.02398v2)", "Abbr": "Personax", "Year": 2025, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2503.02398v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Llm-powered multi-agent framework for goal-oriented learning in intelligent tutoring system](http://arxiv.org/abs/2502.16613v1)", "Abbr": "GenMentor", "Year": 2025, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2502.16613v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Multi-agent collaboration mechanisms based on distributed online meta-learning for mass personalization](http://arxiv.org/abs/2411.07094v2)", "Abbr": NaN, "Year": 2025, "Source": "JIII", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2411.07094v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-Agent LLMs](http://arxiv.org/abs/2411.14739v1)", "Abbr": "Agent4Ranking", "Year": 2025, "Source": "TOIS", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2411.14739v1.pdf)", "Status": "✅ 成功"}
{"Title": "[One Size doesn't Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction](http://arxiv.org/abs/2502.12633v2)", "Abbr": "PACE", "Year": 2025, "Source": "WWW", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2502.12633v2.pdf)", "Status": "✅ 成功"}
{"Title": "[MAP: Multi-user Personalization with Collaborative LLM-powered Agents](http://arxiv.org/abs/2506.11803v2)", "Abbr": "MAP", "Year": 2025, "Source": "CHI", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2506.11803v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Evaluating personalized tool-augmented llms from the perspectives of personalization and proactivity](http://arxiv.org/abs/2503.00771v2)", "Abbr": NaN, "Year": 2025, "Source": "arxiv", "Category": "Agent", "Tag": "Agent", "PDF": "[PDF](http://arxiv.org/pdf/2503.00771v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Align on the fly: Adapting chatbot behavior to established norms](http://arxiv.org/abs/2312.15907v1)", "Abbr": "OPO", "Year": 2023, "Source": "arxiv", "Category": "Memory", "Tag": "memory", "PDF": "[PDF](http://arxiv.org/pdf/2312.15907v1.pdf)", "Status": "✅ 成功"}
{"Title": "[LLM-based medical assistant personalization with short-and long-term memory coordination](http://arxiv.org/abs/2309.11696v3)", "Abbr": "MALP", "Year": 2023, "Source": "arxiv", "Category": "Memory", "Tag": "memory", "PDF": "[PDF](http://arxiv.org/pdf/2309.11696v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Memory-assisted prompt editing to improve GPT-3 after deployment](http://arxiv.org/abs/2201.06009v7)", "Abbr": "MemPrompt", "Year": 2022, "Source": "arxiv", "Category": "Memory", "Tag": "memory", "PDF": "[PDF](http://arxiv.org/pdf/2201.06009v7.pdf)", "Status": "✅ 成功"}
{"Title": "[Fdlora: Personalized federated learning of large language model via dual lora tuning](http://arxiv.org/abs/2406.07925v1)", "Abbr": "Fdlora", "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2406.07925v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Democratizing large language models via personalized parameter-efficient fine-tuning](http://arxiv.org/abs/2402.04401v3)", "Abbr": NaN, "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2402.04401v3.pdf)", "Status": "✅ 成功"}
{"Title": "[Persoma: Personalized soft prompt adapter architecture for personalized language prompting](http://arxiv.org/abs/2408.00960v1)", "Abbr": "Persoma", "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2408.00960v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](http://arxiv.org/abs/2309.14726v2)", "Abbr": NaN, "Year": 2025.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2309.14726v2.pdf)", "Status": "✅ 成功"}
{"Title": "[FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](http://arxiv.org/abs/2508.04698v1)", "Abbr": "FaST", "Year": 2025.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2508.04698v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized large language models through parameter efficient fine-tuning techniques](http://arxiv.org/abs/2506.05316v1)", "Abbr": NaN, "Year": 2024.0, "Source": "SIGIR", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2506.05316v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized pieces: Efficient personalized large language models through collaborative efforts](http://arxiv.org/abs/2406.10471v3)", "Abbr": "PER-PCS", "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2406.10471v3.pdf)", "Status": "✅ 成功"}
{"Title": "[DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized Diffusion Models](http://arxiv.org/abs/2502.05895v1)", "Abbr": "DiffuseKronA", "Year": 2024.0, "Source": "WACV", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2502.05895v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Improving Personalized Sentiment Representation with Knowledge-enhanced and Parameter-efficient Layer Normalization](http://arxiv.org/abs/1810.06645v1)", "Abbr": "E2LN", "Year": 2024.0, "Source": "COLING", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/1810.06645v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Customizing large language model generation style using parameter-efficient finetuning](http://arxiv.org/abs/2409.04574v1)", "Abbr": NaN, "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2409.04574v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalize Your LLM: Fake it then Align it](http://arxiv.org/abs/2503.01048v3)", "Abbr": "CHAMELEON", "Year": 2025.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2503.01048v3.pdf)", "Status": "✅ 成功"}
{"Title": "[FedMCP: parameter-efficient federated learning with model-contrastive personalization](http://arxiv.org/abs/2206.13190v1)", "Abbr": "FedMCP", "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2206.13190v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Efficient model-agnostic alignment via bayesian persuasion](http://arxiv.org/abs/2405.18718v1)", "Abbr": NaN, "Year": 2024.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "PEFT", "PDF": "[PDF](http://arxiv.org/pdf/2405.18718v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Do llms understand user preferences? evaluating llms on user rating prediction](http://arxiv.org/abs/2305.06474v1)", "Abbr": NaN, "Year": 2023.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "FULL", "PDF": "[PDF](http://arxiv.org/pdf/2305.06474v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Teach LLMs to Personalize--An Approach inspired by Writing Education](http://arxiv.org/abs/2308.07968v1)", "Abbr": NaN, "Year": 2023.0, "Source": "arxiv", "Category": "Fine-tune", "Tag": "FULL", "PDF": "[PDF](http://arxiv.org/pdf/2308.07968v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized steering of large language models: Versatile steering vectors through bi-directional preference optimization](http://arxiv.org/abs/2406.00045v2)", "Abbr": NaN, "Year": 2024, "Source": "NIPS", "Category": "Embedding Learning", "Tag": NaN, "PDF": "[PDF](http://arxiv.org/pdf/2406.00045v2.pdf)", "Status": "✅ 成功"}
{"Title": "[User-llm: Efficient llm contextualization with user embeddings](http://arxiv.org/abs/2402.13598v2)", "Abbr": "UserLLM", "Year": 2024, "Source": "arxiv", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2402.13598v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Distributional preference learning: Understanding and accounting for hidden context in RLHF](http://arxiv.org/abs/2312.08358v2)", "Abbr": NaN, "Year": 2024, "Source": "ICLR", "Category": "Embedding Learning", "Tag": "Latent Variable Encoding", "PDF": "[PDF](http://arxiv.org/pdf/2312.08358v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Enhancing social media personalization: dynamic user profile embeddings and multimodal contextual analysis using transformer models](http://arxiv.org/abs/2407.07925v1)", "Abbr": NaN, "Year": 2024, "Source": "arxiv", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2407.07925v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized query expansion with contextual word embeddings](http://arxiv.org/abs/2103.05256v1)", "Abbr": NaN, "Year": 2023, "Source": "TOIS", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2103.05256v1.pdf)", "Status": "✅ 成功"}
{"Title": "[User embedding model for personalized language prompting](http://arxiv.org/abs/2408.00960v1)", "Abbr": "UEM", "Year": 2024, "Source": "arxiv", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2408.00960v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Knowledge-augmented large language models for personalized contextual query suggestion](http://arxiv.org/abs/2311.06318v2)", "Abbr": NaN, "Year": 2024, "Source": "WWW", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2311.06318v2.pdf)", "Status": "✅ 成功"}
{"Title": "[DLVGen: A Dual Latent Variable Approach to Personalized Dialogue Generation](http://arxiv.org/abs/2111.11363v1)", "Abbr": NaN, "Year": 2022, "Source": "ICAART", "Category": "Embedding Learning", "Tag": "Latent Variable Encoding", "PDF": "[PDF](http://arxiv.org/pdf/2111.11363v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control](http://arxiv.org/abs/2310.18342v1)", "Abbr": NaN, "Year": 2023, "Source": "EMNLP", "Category": "Embedding Learning", "Tag": "Latent Variable Encoding", "PDF": "[PDF](http://arxiv.org/pdf/2310.18342v1.pdf)", "Status": "✅ 成功"}
{"Title": "[PIE: A Personalized Information Embedded model for text-based depression detection](http://arxiv.org/abs/2408.03648v1)", "Abbr": "PIE", "Year": 2024, "Source": "IPM", "Category": "Embedding Learning", "Tag": "contextual embedding", "PDF": "[PDF](http://arxiv.org/pdf/2408.03648v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Morpheus: Modeling role from personalized dialogue history by exploring and utilizing latent space](http://arxiv.org/abs/2407.02345v1)", "Abbr": "Morpheus", "Year": 2024, "Source": "arxiv", "Category": "Embedding Learning", "Tag": "Latent Variable Encoding", "PDF": "[PDF](http://arxiv.org/pdf/2407.02345v1.pdf)", "Status": "✅ 成功"}
{"Title": "[RLHF fine-tuning of LLMs for alignment with implicit user feedback in conversational recommenders](http://arxiv.org/abs/2508.05289v1)", "Abbr": NaN, "Year": 2025.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2508.05289v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized language modeling from personalized human feedback](http://arxiv.org/abs/2508.10695v1)", "Abbr": "P-RLHF", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2508.10695v1.pdf)", "Status": "✅ 成功"}
{"Title": "[MaxMin-RLHF: Alignment with diverse human preferences](http://arxiv.org/abs/2405.14705v1)", "Abbr": "MaxMin-RLHF", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2405.14705v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalizing reinforcement learning from human feedback with variational preference learning](http://arxiv.org/abs/2402.05133v3)", "Abbr": NaN, "Year": 2024.0, "Source": "NIPS", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2402.05133v3.pdf)", "Status": "✅ 成功"}
{"Title": "[A Shared Low-Rank Adaptation Approach to Personalized RLHF](http://arxiv.org/abs/2503.19201v1)", "Abbr": NaN, "Year": 2025.0, "Source": "AISTATS", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2503.19201v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Fedrlhf: A convergence-guaranteed federated framework for privacy-preserving and personalized rlhf](http://arxiv.org/abs/2412.15538v2)", "Abbr": "FedRLHF", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2412.15538v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Rlhf from heterogeneous feedback via personalization and preference aggregation](http://arxiv.org/abs/2405.00254v2)", "Abbr": NaN, "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2405.00254v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Value augmented sampling for language model alignment and personalization](http://arxiv.org/abs/2405.06639v1)", "Abbr": NaN, "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2405.06639v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized soups: Personalized large language model alignment via post-hoc parameter merging](http://arxiv.org/abs/2310.11564v1)", "Abbr": "RLPHF", "Year": 2023.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2310.11564v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Fine-grained human feedback gives better rewards for language model training](http://arxiv.org/abs/2306.01693v2)", "Abbr": "Fine-grained RLHF", "Year": 2023.0, "Source": "NIPS", "Category": "Human Feedback", "Tag": "RLHF", "PDF": "[PDF](http://arxiv.org/pdf/2306.01693v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Robust Multi-Objective Preference Alignment with Online DPO](http://arxiv.org/abs/2406.05534v1)", "Abbr": "MO-ODPO", "Year": 2025.0, "Source": "AAAI", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2406.05534v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Beyond one-preference-for-all: Multi-objective direct preference optimization](http://arxiv.org/abs/2508.07638v1)", "Abbr": "MODPO", "Year": 2023.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2508.07638v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Instantly learning preference alignment via in-context DPO](http://arxiv.org/abs/2505.01706v1)", "Abbr": NaN, "Year": 2025.0, "Source": "NAACL", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2505.01706v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Towards robust alignment of language models: Distributionally robustifying direct preference optimization](http://arxiv.org/abs/2407.07880v2)", "Abbr": "Dr. DPO", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2407.07880v2.pdf)", "Status": "✅ 成功"}
{"Title": "[Multi-Preference Lambda-weighted Listwise DPO for Dynamic Preference Alignment](http://arxiv.org/abs/2510.01540v1)", "Abbr": NaN, "Year": 2025.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2510.01540v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Personalized language modeling from personalized human feedback](http://arxiv.org/abs/2508.10695v1)", "Abbr": "P-DPO", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2508.10695v1.pdf)", "Status": "✅ 成功"}
{"Title": "[DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization](http://arxiv.org/abs/2507.01479v1)", "Abbr": "DreamBoothDPO", "Year": 2025.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2507.01479v1.pdf)", "Status": "✅ 成功"}
{"Title": "[alpha-DPO: Adaptive Reward Margin is What Direct Preference Optimization Needs](http://arxiv.org/abs/2502.08922v1)", "Abbr": "alpha-DPO", "Year": 2024.0, "Source": "arxiv", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/2502.08922v1.pdf)", "Status": "✅ 成功"}
{"Title": "[β-DPO: Direct Preference Optimization with Dynamic β](http://arxiv.org/abs/1812.05748v4)", "Abbr": "β-DPO", "Year": 2024.0, "Source": "NIPS", "Category": "Human Feedback", "Tag": "DPO", "PDF": "[PDF](http://arxiv.org/pdf/1812.05748v4.pdf)", "Status": "✅ 成功"}
{"Title": "[Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning](http://arxiv.org/abs/2507.20335v1)", "Abbr": NaN, "Year": 2025, "Source": "arxiv", "Category": "RL", "Tag": "GRPO", "PDF": "[PDF](http://arxiv.org/pdf/2507.20335v1.pdf)", "Status": "✅ 成功"}
{"Title": "[GroupAligner: A Deep Reinforcement Learning with Domain Adaptation for Social Group Alignment](http://arxiv.org/abs/1812.07452v1)", "Abbr": NaN, "Year": 2023, "Source": "WWW", "Category": "RL", "Tag": "deep-reinforment learning", "PDF": "[PDF](http://arxiv.org/pdf/1812.07452v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Optimizing Safe and Aligned Language Generation: A Multi-Objective GRPO Approach](http://arxiv.org/abs/2503.21819v1)", "Abbr": NaN, "Year": 2025, "Source": "arxiv", "Category": "RL", "Tag": "GRPO", "PDF": "[PDF](http://arxiv.org/pdf/2503.21819v1.pdf)", "Status": "✅ 成功"}
{"Title": "[Fine-Tuning a Large Language Model with Reinforcement Learning for Educational Question Generation](http://arxiv.org/abs/2212.03869v1)", "Abbr": NaN, "Year": 2024, "Source": "AIED", "Category": "RL", "Tag": "on-policy RL", "PDF": "[PDF](http://arxiv.org/pdf/2212.03869v1.pdf)", "Status": "✅ 成功"}
{"Title": "[From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](http://arxiv.org/abs/2505.15607v2)", "Abbr": NaN, "Year": 2025, "Source": "arxiv", "Category": "RL", "Tag": "on-policy RL", "PDF": "[PDF](http://arxiv.org/pdf/2505.15607v2.pdf)", "Status": "✅ 成功"}
