import jsonlines
import pandas as pd
from collections import Counter
import requests
from xml.etree import ElementTree as ET
import urllib.parse
import time

# ================== é…ç½® ==================
file_path = '/Users/tom/Downloads/personalized_papers.xlsx'  # ä¿®æ”¹ä¸ºä½ çš„è·¯å¾„
target_columns = ['paper', 'abbr', 'year', 'source', 'tag']

# å­˜å‚¨ç»“æœï¼šæŒ‰ category (sheet_name) åˆ†ç»„
results_by_category = {}

# arXiv API æŸ¥è¯¢å‡½æ•°
def get_arxiv_link(title, retries=2, delay=1):
    """
    æ ¹æ®è®ºæ–‡æ ‡é¢˜æŸ¥è¯¢ arXivï¼Œè¿”å›ç¬¬ä¸€ç¯‡åŒ¹é…çš„ç»“æœé“¾æ¥
    """
    query = urllib.parse.quote_plus(title.strip())
    url = f"http://export.arxiv.org/api/query?search_query={query}&max_results=1"

    headers = {
        "User-Agent": "ResearchBot/1.0 (Academic Tool; https://example.com)"
    }

    for _ in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code != 200:
                time.sleep(delay)
                continue

            root = ET.fromstring(response.content)
            ns = {'atom': 'http://www.w3.org/2005/Atom'}

            entries = root.findall('atom:entry', ns)
            if not entries:
                return None, None

            entry = entries[0]
            paper_url = entry.find('atom:id', ns).text
            pdf_url = paper_url.replace("/abs/", "/pdf/") + ".pdf"
            return paper_url, pdf_url

        except Exception as e:
            print(f"  ğŸ” é‡è¯•ä¸­... é”™è¯¯: {e}")
            time.sleep(delay)
    return None, None


# ================== ä¸»ç¨‹åº ==================
with pd.ExcelFile(file_path) as xls:
    for sheet_name in xls.sheet_names:
        try:
            # è¯»å–åˆ—ååˆ¤æ–­æ˜¯å¦åŒ¹é…
            df_header = pd.read_excel(xls, sheet_name=sheet_name, nrows=0)
            columns = df_header.columns.str.strip().tolist()

            if columns == target_columns:
                print(f"âœ… å¤„ç†åŒ¹é…çš„ sheet: {sheet_name}")
                df = pd.read_excel(xls, sheet_name=sheet_name)
                df.columns = df.columns.str.strip()  # æ¸…ç†åˆ—åç©ºæ ¼

                # åˆå§‹åŒ–è¯¥ category çš„åˆ—è¡¨
                category = sheet_name
                if category not in results_by_category:
                    results_by_category[category] = []

                # éå†æ¯ä¸€è¡Œ
                for _, row in df.iterrows():
                    paper = row['paper']
                    abbr = row.get('abbr', '') or ''
                    year = row.get('year', '')
                    source = row.get('source', '')
                    tag = row.get('tag', '')

                    if pd.isna(paper) or not str(paper).strip():
                        continue  # è·³è¿‡ç©ºæ ‡é¢˜

                    print(f"ğŸ” æŸ¥è¯¢: {paper}")

                    # æŸ¥è¯¢ arXiv
                    abs_url, pdf_url = get_arxiv_link(paper)

                    if abs_url:
                        status = "âœ… æˆåŠŸ"
                    else:
                        abs_url = "#"
                        pdf_url = "#"
                        status = "âŒ æœªæ‰¾åˆ°"

                    # æ„å»ºæ¡ç›®
                    item = {
                        "Title": f"[{paper}]({abs_url})",
                        "Abbr": abbr,
                        "Year": year,
                        "Source": source,
                        "Category": category,
                        "Tag": tag,
                        "PDF": f"[PDF]({pdf_url})" if pdf_url and pdf_url != "#" else "â€“",
                        "Status": status
                    }

                    # æ·»åŠ åˆ°å½“å‰ category ç»„
                    results_by_category[category].append(item)

                    # è¿½åŠ å†™å…¥ JSONLï¼ˆæ¯æ¡ç‹¬ç«‹è®°å½•ï¼‰
                    with jsonlines.open('./papers.jsonl', 'a') as f:
                        f.write(item)

                    # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                    time.sleep(1.5)

        except Exception as e:
            print(f"âš ï¸ è¯»å– sheet {sheet_name} å‡ºé”™: {e}")

# ================== ç”Ÿæˆ Markdown è¾“å‡ºï¼ˆæŒ‰ category åˆ†ç»„ï¼‰==================
if results_by_category:
    md_lines = []

    for category, items in results_by_category.items():
        if not items:
            continue

        # æ·»åŠ å¤§æ ‡é¢˜
        md_lines.append(f"# {category}")
        md_lines.append("")
        md_lines.append("| è®ºæ–‡æ ‡é¢˜ | ç¼©å†™ | å¹´ä»½ | æ¥æº | æ ‡ç­¾ | ä¸‹è½½ | çŠ¶æ€ |")
        md_lines.append("|----------|------|------|------|------|--------|--------|")

        for r in items:
            md_lines.append(
                f"| {r['Title']} | {r['Abbr']} | {r['Year']} | {r['Source']} | {r['Tag']} | {r['PDF']} | {r['Status']} |"
            )
        md_lines.append("")  # ç©ºè¡Œåˆ†éš”ä¸åŒ category

    markdown_output = "\n".join(md_lines)

    # æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + "=" * 60)
    print(markdown_output)

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open("papers.md", "w", encoding="utf-8") as f:
        f.write(markdown_output)
    print(f"\nğŸ“„ å·²ä¿å­˜ä¸º papers.md ï¼ˆå…± {sum(len(v) for v in results_by_category.values())} ç¯‡è®ºæ–‡ï¼‰")

else:
    print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•è®ºæ–‡æ•°æ®ã€‚")

